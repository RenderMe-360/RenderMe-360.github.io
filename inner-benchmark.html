<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>RenderMe-360 - Benchmark</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Poppins:300,400,500,700" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">


  <!--
  <link rel="stylesheet" href="assets/css/bulma.min.css">
-->
  <link rel="stylesheet" href="assets/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="assets/css/bulma-slider.min.css">
  <link rel="stylesheet" href="assets/css/fontawesome.all.min.css">
  <link rel="stylesheet"href="assets/css/academicons.min.css">
  <link rel="stylesheet" href="ass/css/index.css">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Regna
  * Updated: Mar 10 2023 with Bootstrap v5.2.3
  * Template URL: https://bootstrapmade.com/regna-bootstrap-onepage-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex align-items-center header-transpart">
    <div class="container d-flex justify-content-between align-items-center">

      <div id="logo">
        <!--<a href="index.html"><img src="assets/img/logo.png" alt=""></a>-->
        <!-- Uncomment below if you prefer to use a text logo -->
        <h1><a href="index.html">RenderMe-360</a></h1>
      </div>

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto" href="index.html#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="index.html#about">Overview</a></li>
          <li><a class="nav-link scrollto" href="index.html#features-comparison">Core Features</a></li>
          <li><a class="nav-link scrollto" href="index.html#features-comparison2">Statistics</a></li>
         <li class="dropdown"><a href="inner-details.html#"><span>Features</span> <i class="bi bi-chevron-down"></i></a>
            <ul>
              <li><a href="inner-details.html#Overall">Overall</a></li>
              <li><a href="inner-details-Hifi.html#Fidelity">High Fidelity</a></li>
              <li><a href="inner-details-diversity.html#Diversity">High Diversity</a></li>
              <li><a href="inner-details-richannotation.html#Annotation">Rich Annotation</a></li>
            </ul>
          </li>
          <!-- <li><a class="nav-link scrollto" href="#services">Services</a></li>
          <li><a class="nav-link scrollto " href="#portfolio">Portfolio</a></li>
         <li><a class="nav-link scrollto" href="#team">Team</a></li>-->
         <li class="dropdown"><a href="inner-benchmark.html"><span>Benchmark</span> <i class="bi bi-chevron-down"></i></a>
          <ul>
            <!--
            <li class="dropdown"><a href="#"><span>Deep Drop Down</span> <i class="bi bi-chevron-right"></i></a>
              <ul>
                <li><a href="#">Deep Drop Down 1</a></li>
                <li><a href="#">Deep Drop Down 2</a></li>
                <li><a href="#">Deep Drop Down 3</a></li>
                <li><a href="#">Deep Drop Down 4</a></li>
                <li><a href="#">Deep Drop Down 5</a></li>
              </ul>
            </li>-->
            <li><a href="inner-benchmark.html#Benchmark Overview">Benchmark Overview</a></li>
            <li><a href="inner-benchmark-nvs.html#NVS">NVS</a></li>
            <li><a href="inner-benchmark-nes.html#NES">Novel Expression</a></li><!--href="inner-benchmark-novelpose.html#Novel Pose-->
            <li><a href="inner-benchmark-talkinghead.html#Talking Head">Talking Head</a></li>
            <li><a href="inner-benchmark-hairediting.html#Hair Editing">Hair Editing</a></li><!--#inner-benchmark-hairediting.html#Hair Editing-->
            <li><a href="inner-benchmark-nis.html#Generlization">Generlization</a></li>
          </ul>
        </li>
        </li>

          <!--<li><a class="nav-link scrollto" href="inner-gallery.html">Gallery</a></li>-->
          <!--<li class="dropdown"><a href=inner-page.html><span>Gallery</span> <i class="bi bi-chevron-down"></i></a>-->
            <li><a class="nav-link scrollto" >Download</a></li>
          </li>

        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->
    </div>
  </header><!-- End Header -->


  <main id="main">

    <!-- ======= Breadcrumbs Section ======= -->
    <section class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <h2>Benchmark</h2>
          <ol>
            <li><a href="index.html">Home</a></li>
            <li>Benchmark</li>
          </ol>
        </div>

      </div>
    </section><!-- End Breadcrumbs Section -->

    <section class="inner-page pt-4">
      <div class="container">
        <!--
        <p>
          xxx
        </p>-->
      </div>
    </section>


    <section id="features-comparison4">

        <div class="container">
          <div class="section-header">
            <h3 class="section-title2">Benchmark</h3>

            <span class="section-divider"></span>
           <!-- <p class="section-description">TDW has been used in a number of labs within MIT and Stanford, as well as IBM</p> -->
          </div>

     
  

                       <center>
                        <div class="container" >
                          <div class="box">
                            <!--<div class="icon"><a href=""><i class="bi bi-briefcase"></i></a></div>-->
                            <h4 class="title"><a href="inner-benchmark-nvs.html">Novel View</a></h4>
                              <a href="inner-benchmark-nvs.html"><img class="listitemimage" src="assets/img/benchmark/nvs.jpg" width="88%"></a>
                            <p class="description"> Novel View Synthesis (NVS) is concerned with synthesizing views under camera viewpoint transformations from one or multiple input images. The tested methods include NGP [SIGGRAPH-2022], NeuS [NeurIPS-2021], NV [SIGGRAPH-2019] and MVP [SIGGRAPH-2021].
                              <dl>
                                <dt>NGP [SIGGRAPH-2022]</dt> 
                                <dd>NGP utilizes multi-resolution hash encoding to automatically focuses on relevant detail by performing a practical learning-based alternative. It is a case specific, radiance field-based method and images conditioning that fed with multi-view images and camera calibration and yield static results with neural volumetric representation. </dd>
                                <dt>NeuS [NeurIPS-2021]</dt>
                                <dd>NeuS is to multiview surface reconstruction that represents 3D surfaces as neural SDF, and it develops a new volume rendering method for training the implicit SDF representation. It is a case specific, SDF-based method and images conditioning that fed with multi-view images and camera calibration and yield static results with neural SDF representation.</dd>
                                <dt>NV [SIGGRAPH-2019]</dt>
                                <dd>NV models objects and scenes with a semi-transparent volume representation that end-to-end learns from multi-view RGB images. It is a case specific, SDF-based method and images conditioning that fed with multi-view images and camera calibration and yield dynamic results with neural volumetric representation.</dd>
                                <dt>MVP [SIGGRAPH-2021]</dt>
                                <dd>MVP is a 3D neural scene representation supervised by 2D and 3D representation, and it generates volumetric and primitive-based paradigms under a unified representation. It is a case specific, radiance field-based and images conditioning method that fed with multi-view images and camera calibration and yield dynamic results with neural volumetric representation. It needs parametric models as facial prois.</dd>
                            </dl> 
                              

                            </p>
                          </div>
                        </div>
                        <div class="container" >
                          <div class="box">
                            <!--<div class="icon"><a href=""><i class="bi bi-card-checklist"></i></a></div>-->
                            <h4 class="title"><a href="inner-benchmark-nes.html">Novel Expression</a></h4>
                              <a href="inner-benchmark-nes.html"><img class="listitemimage" src="assets/img/benchmark/nes.jpg" width="88%"></a>
                            <p class="description">Novel Expression Synthesis (NES) is to generate the new expression of a subject from one expression of the same subject. The tested methods include NeRFace [CVPR-2021], IM Avatar [CVPR-2022], and PointAvatar [Arxiv-2022].
                              <dl>
                                <dt>NeRFace [CVPR-2021]</dt> 
                                <dd>NeRFace renders controllable 4D facial avatars based on dynamic neural radiance fields using volumetric rendering. It is a case specific, radiance field-based method and latent codes conditioning that fed with single-view images and camera calibration and yield dynamic results with neural volumetric representation. It needs parametric models as facial prois.</dd>
                                <dt>IM Avatar [CVPR-2022]</dt>
                                <dd>IM Avatar is an implicit morphable head avatar, controlled via expression and pose parameters with the ability to model diverse and detailed hairstyles and facial appearance. It is a case specific, radiance field-based method and latent codes conditioning that fed with single-view images and camera calibration and yield dynamic results with neural volumetric representation. It needs parametric models as facial prois.</dd>
                                <dt>PointAvatar [Arxiv-2022]</dt>
                                <dd>PointAvatar is a deformable point-based avatar representation and features high flexibility, efficient rendering and straightforward deformation. It is a case specific, radiance field-based method and latent codes conditioning that fed with single-view images and camera calibration and yield dynamic results with point-based representation. It needs parametric models as facial prois.</dd>
                                
                            </dl> 

                            </p>
                          </div>
                        </div>
              
                        <div class="container" >
                          <div class="box">
                            <!--<div class="icon"><a href=""><i class="bi bi-binoculars"></i></a></div>-->
                            <h4 class="title"><a href="inner-benchmark-talkinghead.html">Talking Head</a></h4>
                              <a href="inner-benchmark-talkinghead.html"><img class="listitemimage" src="assets/img/benchmark/talking.jpg" width="88%"></a>
                            <p class="description">Talking Head is to generate the audio-driven head video. The tested methods include AD-NeRF [ICCV-2021], and SSP-NeRF [ECCV-2022].
                              <dl>
                                <dt>AD-NeRF [ICCV-2021]</dt> 
                                <dd>AD-NeRF uses volume rendering on two elaborately designed NeRFs to directly synthesize human head and upper body from audio signal without relying on intermediate representations. It is a case specific, radiance field-based method and latent codes conditioning that fed with single-view images and camera calibration and yield dynamic results with neural volumetric representation. It needs parametric models as facial prois. </dd>
                                <dt>SSP-NeRF [ECCV-2022]</dt>
                                <dd>SSP-NeRF introduces semantic-aware dynamic ray sampling module and torso deformation module for audio-driven portrait generation. It is a case specific, radiance field-based method and latent codes conditioning that fed with single-view images and camera calibration and yield dynamic results with neural volumetric representation. It needs parametric models as facial prois.</dd>
                                
                            </dl> 
                              
                            </p>
                          </div>
                        </div>
              
              
                        <div class="container" >
                          <div class="box">
                            <!--<div class="icon"><a href=""><i class="bi bi-binoculars"></i></a></div>-->
                            <h4 class="title"><a href="inner-benchmark-hairediting.html">Hair Editing</a></h4>
                              <a href="inner-benchmark-hairediting.html"><img class="listitemimage" src="assets/img/benchmark/hair.jpg" width="88%"></a>
                            <p class="description">Hair Editing is to change the hair styles of the original subjects. The tested methods include HairCLIP [CVPR-2022] and StyleCLIP [ICCV-2021]. The pre-processing inversion methods of the two methods involves e4e [SIGGRAPH-2021], PTI [SIGGRAPH-2021], Restyle [ICCV-2021], and HyperStyle [CVPR-2022].
                              <dl>
                                <dt>HairCLIP [CVPR-2022]</dt> 
                                <dd>HairCLIP individually or jointly provides textual descriptions and reference images to complete the hair editing with CLIP. It is a generlizable, convolution-based method and feature space conditioning that fed with single-view images and camera calibration and yield dynamic results with neural volumetric representation. It needs parametric models as facial prois. </dd>
                                <dt>StyleCLIP [ICCV-2021]</dt>
                                <dd>StyleCLIP introduces three image manipulation methods that combine StyleGAN and CLIP. It is a generlizable, convolution-based method and feature space conditioning that fed with single-view images and camera calibration and yield dynamic results with neural volumetric representation. It needs parametric models as facial prois.</dd>
                                
                                
                            </dl> 

                            </p>
              
                          </div>
                        
                          <div class="container" >
                            <div class="box">
                              <!--<div class="icon"><a href=""><i class="bi bi-binoculars"></i></a></div>-->
                              <h4 class="title"><a href="inner-benchmark-nis.html">Generlization</a></h4>
                                <a href="inner-benchmark-nis.html"><img class="listitemimage" src="assets/img/benchmark/nis.jpg" width="88%"></a>
                              <p class="description">Generlization of Novel ID Synthesis and Novel view Synthesis aims to synthesize the novel expressions of novel identities. The tested methods of generalization ability include  IBRNet [CVPR-2021], VisionNeRF [WACV-2023], and KeypointNeRF [ECCV-2022].

                                <dl>
                                  <dt>IBRNet [CVPR-2021]</dt> 
                                  <dd>IBRNet blends pixels from nearby images with weights and volume densities inferred by a network comprising an MLP and ray transformer. It is a generlizable, radiance field-based method and images conditioning that fed with multi-view images and camera calibration and yield static results with neural volumetric representation. </dd>
                                  <dt>VisionNeRF [WACV-2023]</dt> 
                                  <dd>VisionNeRF applies vision transformer in conjunction with convolutional networks to extract global and local features as 3D representations. It is a generlizable, radiance field-based method and images conditioning that fed with multi-view images and camera calibration and yield static results with neural volumetric representation. </dd>
                                  <dt>KeypointNeRF [ECCV-2022]</dt> 
                                  <dd>KeypointNeRF is a novel spatial encoding based on relative information extracted from 3D keypoints. It is a generlizable, radiance field-based method and images conditioning that fed with multi-view images and camera calibration and yield static results with neural volumetric representation. It needs facial keypoints as facial prois.</dd>
                                  
                              </dl> 
                              </p>
                
                            </div>

                        </div>
              
                      </center>
  </script>

</body>

</html>
